{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiVFe6EGLtpdmHP36kGjLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadHelmyOmar/ArabicPIIRedaction/blob/main/data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "vz0XTsAGP88t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data"
      ],
      "metadata": {
        "id": "Rc7U_0COLFBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nSbGSaxnLEcb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "wrT1bfJee2ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lf2iWcP3dK3X",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# All Data\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/ArabicPIIRedaction/data/ALL_DATA.csv\"\n",
        "\n",
        "train = pd.read_csv(data_path)"
      ],
      "metadata": {
        "id": "CMW4N-ORNfzY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train))\n",
        "train.head()"
      ],
      "metadata": {
        "id": "5VXlg-xXcADE",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train['dialect'].unique())\n",
        "print(train['dialect'].value_counts())"
      ],
      "metadata": {
        "id": "A9zpMNEjcgSt",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train.loc[train['dialect']=='0', 'dialect'] = 'eg'\n",
        "print(train['dialect'].value_counts())"
      ],
      "metadata": {
        "id": "PnKuov5yee9u",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JEGe0Bq2341A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmented with Arabic names and locations\n",
        "\n",
        "names_loc_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ArabicPIIRedaction/data/names_locations_augmented_data.csv\")\n",
        "\n",
        "print(len(names_loc_data))\n",
        "names_loc_data.head()"
      ],
      "metadata": {
        "id": "dM5lBZ5Ce-Gq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train, names_loc_data], ignore_index=True)\n",
        "print(len(train))\n",
        "train['dialect'].value_counts()"
      ],
      "metadata": {
        "id": "eLgPsDJ8fp2p",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the data\n",
        "\n",
        "train = train.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "8WYi3OUFkFUI",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning and preprocessing"
      ],
      "metadata": {
        "id": "COAdxUm1jBNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "from IPython.display import display, HTML\n",
        "import re\n",
        "import ast"
      ],
      "metadata": {
        "trusted": true,
        "id": "UUXFYcs71Iql"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0, len(train))\n",
        "idx"
      ],
      "metadata": {
        "id": "cP_-UOWLdGiY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.tokens[idx])\n",
        "print(type(train.tokens[idx]))\n",
        "print(train.tags[idx])\n",
        "print(type(train.tags[idx]))\n",
        "print(len(train.tokens[idx]))\n",
        "print(len(train.tags[idx]))"
      ],
      "metadata": {
        "id": "cOTox6jAdT_z",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_list = ast.literal_eval(train.tokens[idx])\n",
        "print(tokens_list)\n",
        "print(type(tokens_list), len(tokens_list))\n",
        "\n",
        "tags_list = ast.literal_eval(train.tags[idx])\n",
        "print(tags_list)\n",
        "print(type(tags_list), len(tags_list))"
      ],
      "metadata": {
        "id": "3bLD3jmHe0NH",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens and tags columns to lists\n",
        "\n",
        "train['tokens'] = train[\"tokens\"].apply(ast.literal_eval)\n",
        "train['tags'] = train['tags'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "4JHYoNc5eyeQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.tokens[idx])\n",
        "print(type(train.tokens[idx]))\n",
        "print(train.tags[idx])\n",
        "print(type(train.tags[idx]))\n",
        "print(len(train.tokens[idx]))\n",
        "print(len(train.tags[idx]))"
      ],
      "metadata": {
        "id": "Fsn26ncVimAc",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if there is mismatching lengths between tokens and tags\n",
        "\n",
        "length_comparison_result = train.apply(lambda row: len(train.tokens) == len(train.tags), axis=1)\n",
        "print(f\"\\nNumber of rows with mismatching lengths: {sum(~length_comparison_result)}\")"
      ],
      "metadata": {
        "id": "m4vw380mj6wb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tags_to_mask = ['PHONEIMEI',\n",
        " 'VEHICLEVRM',\n",
        " 'LITECOINADDRESS',\n",
        " 'CREDITCARDNUMBER',\n",
        " 'DATE',\n",
        " 'NEARBYGPSCOORDINATE',\n",
        " 'BITCOINADDRESS',\n",
        " 'GENDER',\n",
        " 'PERSONNAME',\n",
        " 'JOBTITLE',\n",
        " 'TIME',\n",
        " 'CURRENCY',\n",
        " 'BIC',\n",
        " 'MASKEDNUMBER',\n",
        " 'STREET',\n",
        " 'MAC',\n",
        " 'DOB',\n",
        " 'SECONDARYADDRESS',\n",
        " 'CREDITCARDISSUER',\n",
        " 'ZIPCODE',\n",
        " 'USERAGENT',\n",
        " 'CURRENCYSYMBOL',\n",
        " 'JOBTYPE',\n",
        " 'BUILDINGNUMBER',\n",
        " 'AGE',\n",
        " 'MIDDLENAME',\n",
        " 'CREDITCARDINUMBER',\n",
        " 'ACCOUNTNUMBER',\n",
        " 'PIN',\n",
        " 'FIRSTNAME',\n",
        " 'ORDINALDIRECTION',\n",
        " 'PASSWORD',\n",
        " 'PHONENUMBER',\n",
        " 'IPV4',\n",
        " 'CREDITCARDCVV',\n",
        " 'USERNAME',\n",
        " 'HEIGHT',\n",
        " 'CURRENCYCODE',\n",
        " 'ACCOUNTNAME',\n",
        " 'IBAN',\n",
        " 'AMOUNT',\n",
        " 'PREFIX',\n",
        " 'VEHICLEVIN',\n",
        " 'SEX',\n",
        " 'EMAIL',\n",
        " 'ETHEREUMADDRESS',\n",
        " 'IPV6',\n",
        " 'SSN',\n",
        " 'URL',\n",
        " 'LASTNAME',\n",
        " 'CURRENCYNAME',\n",
        " 'IP']"
      ],
      "metadata": {
        "id": "WbAa-c_gjSMA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def masker(row, tags_to_mask, discrepancy_list):\n",
        "    \"\"\"\n",
        "    Constructs a masked sentence and records discrepancies, attempting to preserve original formatting.\n",
        "\n",
        "    Args:\n",
        "    row: A pandas DataFrame row with 'tokens', 'tags', and 'clean_source' columns.\n",
        "    tags_to_mask: List of tag entity types to mask.\n",
        "    discrepancy_list: A list to append discrepancy details.\n",
        "\n",
        "    Returns:\n",
        "    The constructed sentence with masked tokens for the given row, attempting to preserve spacing.\n",
        "    \"\"\"\n",
        "    tokens = row['tokens']\n",
        "    tags = row['tags']\n",
        "    clean_source = row['clean_source']\n",
        "    masked_sentence = \"\"\n",
        "    current_position = 0\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        token = tokens[i]\n",
        "        tag = tags[i]\n",
        "\n",
        "        # Find the position of the current token in the original string starting from the last processed position\n",
        "        start_index = clean_source.find(token, current_position)\n",
        "\n",
        "        if start_index != -1:\n",
        "            # Append the text from the current position up to the start of the token\n",
        "            masked_sentence += clean_source[current_position:start_index]\n",
        "\n",
        "            if tag[2:] in tags_to_mask:\n",
        "                masked_sentence += '[MASK]'\n",
        "            else:\n",
        "                masked_sentence += token\n",
        "\n",
        "            # Update the current position to the end of the current token\n",
        "            current_position = start_index + len(token)\n",
        "        else:\n",
        "            # If token not found at or after current_position, it indicates a discrepancy\n",
        "            print(f\"Warning: Token '{token}' not found in clean_source at or after position {current_position}\\n\")\n",
        "\n",
        "            # Record the discrepancy\n",
        "            discrepancy_list.append({\n",
        "                'clean_source': clean_source,\n",
        "                'tokens': tokens,\n",
        "                'tags': tags,\n",
        "                'discrepancy_token': token,\n",
        "                'discrepancy_position': current_position\n",
        "            })\n",
        "\n",
        "            # You might choose how to handle the token that wasn't found -\n",
        "            # here, we'll just append it unmasked to keep the process going\n",
        "            masked_sentence += token\n",
        "            # We don't update current_position based on this token as it wasn't found correctly\n",
        "\n",
        "    # Append any remaining text after the last token\n",
        "    masked_sentence += clean_source[current_position:]\n",
        "    return masked_sentence"
      ],
      "metadata": {
        "id": "uKH8rQGHlwlL",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store discrepancy information\n",
        "discrepancy_data = []\n",
        "\n",
        "# Creating a new masked sentence\n",
        "train['masked_source'] = train.apply(lambda row: masker(row, tags_to_mask, discrepancy_data), axis=1)\n",
        "\n",
        "# Convert the list of discrepancy data into a DataFrame\n",
        "discrepancy_df = pd.DataFrame(discrepancy_data)"
      ],
      "metadata": {
        "id": "bVS8iy2MlyXF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(discrepancy_df)} discrepancies are found\")"
      ],
      "metadata": {
        "id": "Z3tXl7GDuKza",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(discrepancy_df.clean_source.unique())} sentences are detected as discrepancies.\")"
      ],
      "metadata": {
        "id": "U0hUFLbRuMvr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop discrepancies\n",
        "\n",
        "print(f\"Original number of rows: {len(train)}\")\n",
        "\n",
        "train = train[~train['clean_source'].isin(discrepancy_df['clean_source'])]\n",
        "\n",
        "print(f\"Number of rows after dropping discrepancies: {len(train)}\")"
      ],
      "metadata": {
        "id": "5ou3MRUq2aFA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train[['clean_source','masked_source']]"
      ],
      "metadata": {
        "id": "r3XJgWW-l4d4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_final = train[['clean_source', 'masked_source']].copy()\n",
        "train_final.rename(columns={'clean_source': 'source', 'masked_source': 'target'}, inplace = True)\n",
        "\n",
        "# train_final.head()\n",
        "\n",
        "# # Save the train data after the updates\n",
        "# train_final.to_csv('/content/drive/MyDrive/Colab Notebooks/ArabicPIIRedaction/data/masked_train_data.csv', index=False)"
      ],
      "metadata": {
        "id": "6kaaHn4_47KT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ArabicPIIRedaction/data/masked_train_data.csv\")"
      ],
      "metadata": {
        "id": "2ndJMEyxaGSO",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_consecutive_masks(text):\n",
        "    \"\"\"Merges consecutive occurrences of '[MASK]' in a string, preserving a trailing whitespace if present.\"\"\"\n",
        "    return re.sub(r'(\\[MASK\\](\\s*)){2,}', r'[MASK]\\2', text)\n",
        "\n",
        "def display_ar_eng(text):\n",
        "    \"\"\"Display Arabic and English text in a readable format\"\"\"\n",
        "    display(HTML(f'<div dir=\"rtl\" style=\"font-size:18px; line-height:1.8; font-family: \"Arial\", sans-serif;\">{text}</div>'))"
      ],
      "metadata": {
        "id": "1OlV1wIpaGSP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['target'] = train_data['target'].apply(merge_consecutive_masks)\n",
        "\n",
        "display_ar_eng(train_data['source'][0])\n",
        "display_ar_eng(train_data['target'][0])"
      ],
      "metadata": {
        "id": "7xzjyqXTaGSQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the train data after the updates\n",
        "# train_data.to_csv('/content/drive/MyDrive/Colab Notebooks/ArabicPIIRedaction/data/masked_train_data.csv', index=False)"
      ],
      "metadata": {
        "id": "NDW93memaKsN",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}